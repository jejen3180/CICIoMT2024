{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0174f62-728f-4cd6-a2ea-1613bf186de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrastive Learning + Zero-Day Detection for IoMT IDS (Triplet Loss for Better Embedding)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Lambda\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from scipy.spatial import distance\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ed80f6f-ad83-4b18-bdc1-8f5428e31077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load Dataset ===\n",
    "def load_dataset_from_structure(root_path):\n",
    "    data = []\n",
    "    for file in root_path.glob(\"**/*.csv\"):\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            if df.empty: continue\n",
    "            category = file.parent.parent.name\n",
    "            attack = file.parent.name\n",
    "            label_class = 'Benign' if category.upper() == 'BENIGN' else 'Attack'\n",
    "            df['category'] = category\n",
    "            df['attack'] = attack\n",
    "            df['class'] = label_class\n",
    "            data.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to read file {file}: {e}\")\n",
    "    return pd.concat(data, ignore_index=True) if data else pd.DataFrame()\n",
    "\n",
    "train_root = Path('../../../Data/CICIoMT2024/train')\n",
    "test_root = Path('../../../Data/CICIoMT2024/test')\n",
    "\n",
    "train_df = load_dataset_from_structure(train_root)\n",
    "test_df = load_dataset_from_structure(test_root)\n",
    "\n",
    "df = pd.concat([train_df, test_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b66e8c09-6401-444b-a26b-6aff57f26e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Triplet Loss Function ===\n",
    "def triplet_loss(margin=1.0):\n",
    "    def loss(y_true, y_pred):\n",
    "        anchor, positive, negative = y_pred[:, :32], y_pred[:, 32:64], y_pred[:, 64:]\n",
    "        pos_dist = K.sum(K.square(anchor - positive), axis=1)\n",
    "        neg_dist = K.sum(K.square(anchor - negative), axis=1)\n",
    "        return K.mean(K.maximum(pos_dist - neg_dist + margin, 0.0))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fabd75ad-601a-4adc-9371-080b97940714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž Unique attack values: ['BENIGN' 'DDOS ICMP' 'DDOS SYN' 'DDOS TCP' 'DDOS UDP' 'DOS ICMP'\n",
      " 'DOS SYN' 'DOS TCP' 'DOS UDP' 'DDOS CONNECT FLOOD' 'DDOS PUBLISH FLOOD'\n",
      " 'DOS CONNECT FLOOD' 'DOS PUBLISH FLOOD' 'MALFORMED DATA' 'OS SCAN'\n",
      " 'PING SWEEP' 'PORT SCAN' 'RECON VULSCAN' 'SPOOFING']\n",
      "[INFO] valid_cols: 45 features\n",
      "[INFO] triplet_benign shape: (17791, 45)\n",
      "[INFO] triplet_attack shape: (17791, 45)\n",
      "[INFO] triplet_zero shape: (17791, 45)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.94 GiB for an array with shape (8775013, 45) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTidak ada fitur numerik yang cocok di antara triplet datasets. Periksa kembali struktur datanya.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     37\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m---> 38\u001b[0m scaler\u001b[38;5;241m.\u001b[39mfit(df[valid_cols])\n\u001b[0;32m     40\u001b[0m anchor \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(triplet_benign[valid_cols])\n\u001b[0;32m     41\u001b[0m positive \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(triplet_attack[valid_cols])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:878\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 878\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_fit(X, y, sample_weight)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:999\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    996\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_seen_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39misnan(X)\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    998\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 999\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvar_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_seen_ \u001b[38;5;241m=\u001b[39m _incremental_mean_and_var(\n\u001b[0;32m   1000\u001b[0m             X,\n\u001b[0;32m   1001\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_,\n\u001b[0;32m   1002\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvar_,\n\u001b[0;32m   1003\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_seen_,\n\u001b[0;32m   1004\u001b[0m             sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1005\u001b[0m         )\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;66;03m# for backward-compatibility, reduce n_samples_seen_ to an integer\u001b[39;00m\n\u001b[0;32m   1008\u001b[0m \u001b[38;5;66;03m# if the number of samples is the same for each feature (i.e. no\u001b[39;00m\n\u001b[0;32m   1009\u001b[0m \u001b[38;5;66;03m# missing values)\u001b[39;00m\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mptp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_seen_) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1143\u001b[0m, in \u001b[0;36m_incremental_mean_and_var\u001b[1;34m(X, last_mean, last_variance, last_sample_count, sample_weight)\u001b[0m\n\u001b[0;32m   1141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1142\u001b[0m     T \u001b[38;5;241m=\u001b[39m new_sum \u001b[38;5;241m/\u001b[39m new_sample_count\n\u001b[1;32m-> 1143\u001b[0m     temp \u001b[38;5;241m=\u001b[39m X \u001b[38;5;241m-\u001b[39m T\n\u001b[0;32m   1144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1145\u001b[0m         \u001b[38;5;66;03m# equivalent to np.nansum((X-T)**2 * sample_weight, axis=0)\u001b[39;00m\n\u001b[0;32m   1146\u001b[0m         \u001b[38;5;66;03m# safer because np.float64(X*W) != np.float64(X)*np.float64(W)\u001b[39;00m\n\u001b[0;32m   1147\u001b[0m         correction \u001b[38;5;241m=\u001b[39m _safe_accumulator_op(\n\u001b[0;32m   1148\u001b[0m             np\u001b[38;5;241m.\u001b[39mmatmul, sample_weight, np\u001b[38;5;241m.\u001b[39mwhere(X_nan_mask, \u001b[38;5;241m0\u001b[39m, temp)\n\u001b[0;32m   1149\u001b[0m         )\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 2.94 GiB for an array with shape (8775013, 45) and data type float64"
     ]
    }
   ],
   "source": [
    "# === Prepare Data ===\n",
    "df['attack'] = df['attack'].str.upper().str.strip()\n",
    "df['class'] = df['class'].str.upper().str.strip()\n",
    "\n",
    "print(\"ðŸ”Ž Unique attack values:\", df['attack'].unique())\n",
    "\n",
    "zero_attack_label = 'SPOOFING'  # Fokus zero-day spoofing\n",
    "benign_df = df[df['class'] == 'BENIGN'].copy()\n",
    "zero_df = df[df['attack'] == zero_attack_label].copy()\n",
    "attack_df = df[(df['class'] == 'ATTACK') & (df['attack'] != zero_attack_label)].copy()\n",
    "\n",
    "triplet_size = min(len(benign_df), len(zero_df), len(attack_df))\n",
    "if triplet_size == 0:\n",
    "    raise ValueError(\"Triplet datasets are empty. Periksa kembali label 'BENIGN', 'SPOOFING', atau struktur data.\")\n",
    "\n",
    "triplet_benign = benign_df.sample(n=triplet_size, random_state=42)\n",
    "triplet_attack = attack_df.sample(n=triplet_size, random_state=42)\n",
    "triplet_zero = zero_df.sample(n=triplet_size, random_state=42)\n",
    "\n",
    "feature_cols = df.select_dtypes(include='number').columns.tolist()\n",
    "triplet_benign = triplet_benign.dropna(subset=feature_cols)\n",
    "triplet_attack = triplet_attack.dropna(subset=feature_cols)\n",
    "triplet_zero = triplet_zero.dropna(subset=feature_cols)\n",
    "\n",
    "valid_cols = list(set(feature_cols).intersection(\n",
    "    triplet_benign.columns, triplet_attack.columns, triplet_zero.columns\n",
    "))\n",
    "\n",
    "print(f\"[INFO] valid_cols: {len(valid_cols)} features\")\n",
    "print(f\"[INFO] triplet_benign shape: {triplet_benign[valid_cols].shape}\")\n",
    "print(f\"[INFO] triplet_attack shape: {triplet_attack[valid_cols].shape}\")\n",
    "print(f\"[INFO] triplet_zero shape: {triplet_zero[valid_cols].shape}\")\n",
    "\n",
    "if not valid_cols:\n",
    "    raise ValueError(\"Tidak ada fitur numerik yang cocok di antara triplet datasets. Periksa kembali struktur datanya.\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df[valid_cols])\n",
    "\n",
    "anchor = scaler.transform(triplet_benign[valid_cols])\n",
    "positive = scaler.transform(triplet_attack[valid_cols])\n",
    "negative = scaler.transform(triplet_zero[valid_cols])\n",
    "\n",
    "X_triplet = np.concatenate([anchor, positive, negative], axis=1)\n",
    "y_dummy = np.zeros((X_triplet.shape[0],))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef0f0a7-ec07-4fba-a1bb-0c03675d0e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Build Triplet Network ===\n",
    "def build_base_network(input_shape):\n",
    "    inp = Input(shape=(input_shape,))\n",
    "    x = Dense(128, activation='relu')(inp)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(32, activation='linear')(x)\n",
    "    return Model(inp, x)\n",
    "\n",
    "input_shape = anchor.shape[1]\n",
    "base_network = build_base_network(input_shape)\n",
    "\n",
    "anchor_input = Input(shape=(input_shape,), name='anchor_input')\n",
    "positive_input = Input(shape=(input_shape,), name='positive_input')\n",
    "negative_input = Input(shape=(input_shape,), name='negative_input')\n",
    "\n",
    "encoded_anchor = base_network(anchor_input)\n",
    "encoded_positive = base_network(positive_input)\n",
    "encoded_negative = base_network(negative_input)\n",
    "\n",
    "merged_output = Lambda(lambda x: K.concatenate(x, axis=1))([encoded_anchor, encoded_positive, encoded_negative])\n",
    "triplet_model = Model(inputs=[anchor_input, positive_input, negative_input], outputs=merged_output)\n",
    "\n",
    "triplet_model.compile(loss=triplet_loss(margin=1.0), optimizer=Adam(0.001))\n",
    "triplet_model.fit([anchor, positive, negative], y_dummy, batch_size=64, epochs=15, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc085f9-adf8-4886-9322-c9072d14548b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Embedding dan Deteksi Anomali ===\n",
    "encoder_model = base_network\n",
    "embedding_benign = encoder_model.predict(scaler.transform(benign_df[valid_cols]))\n",
    "embedding_zero = encoder_model.predict(scaler.transform(zero_df[valid_cols]))\n",
    "\n",
    "# Isolation Forest\n",
    "iso = IsolationForest(contamination=0.05, random_state=42)\n",
    "iso.fit(embedding_benign)\n",
    "iso_pred = [1 if p == -1 else 0 for p in np.concatenate([iso.predict(embedding_benign), iso.predict(embedding_zero)])]\n",
    "\n",
    "# One-Class SVM\n",
    "svm = OneClassSVM(kernel='rbf', gamma='auto')\n",
    "svm.fit(embedding_benign)\n",
    "svm_pred = [1 if p == -1 else 0 for p in np.concatenate([svm.predict(embedding_benign), svm.predict(embedding_zero)])]\n",
    "\n",
    "# Mahalanobis Distance\n",
    "mean_vec = np.mean(embedding_benign, axis=0)\n",
    "cov_inv = np.linalg.pinv(np.cov(embedding_benign, rowvar=False))\n",
    "d_mahal_benign = [distance.mahalanobis(x, mean_vec, cov_inv) for x in embedding_benign]\n",
    "d_mahal_zero = [distance.mahalanobis(x, mean_vec, cov_inv) for x in embedding_zero]\n",
    "thresh = np.percentile(d_mahal_benign, 95)\n",
    "mahal_pred = [1 if d > thresh else 0 for d in d_mahal_benign + d_mahal_zero]\n",
    "\n",
    "# Ensemble\n",
    "ensemble_pred = [1 if (s == 1 and m == 1) else 0 for s, m in zip(svm_pred, mahal_pred)]\n",
    "y_true = [0] * len(embedding_benign) + [1] * len(embedding_zero)\n",
    "\n",
    "print(\"\\n[Ensemble Detection Evaluation]\")\n",
    "print(classification_report(y_true, ensemble_pred, target_names=['Benign', zero_attack_label]))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_true, ensemble_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
