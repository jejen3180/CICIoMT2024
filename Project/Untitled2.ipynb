{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "317d0d48-8735-4479-858b-03ce3336b3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrastive Learning + Zero-Day Detection for IoMT IDS (Memperbaiki SPOOFING)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca0a91ca-dadc-4580-a67d-3fbb8c65c363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (7160831, 48)\n",
      "category  attack              class \n",
      "DDoS      DDoS UDP            Attack    1635956\n",
      "          DDoS ICMP           Attack    1537476\n",
      "          DDoS TCP            Attack     804465\n",
      "          DDoS SYN            Attack     801962\n",
      "DoS       DoS UDP             Attack     566950\n",
      "          DoS SYN             Attack     441903\n",
      "          DoS ICMP            Attack     416292\n",
      "          DoS TCP             Attack     380384\n",
      "BENIGN    BENIGN              Benign     192732\n",
      "MQTT      DDoS Connect Flood  Attack     173036\n",
      "RECON     Port Scan           Attack      83981\n",
      "MQTT      DoS Publish Flood   Attack      44376\n",
      "          DDoS Publish Flood  Attack      27623\n",
      "RECON     OS Scan             Attack      16832\n",
      "SPOOFING  SPOOFING            Attack      16047\n",
      "MQTT      DoS Connect Flood   Attack      12773\n",
      "          Malformed Data      Attack       5130\n",
      "RECON     Recon VulScan       Attack       2173\n",
      "          Ping Sweep          Attack        740\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test set: (1614182, 48)\n",
      "category  attack              class \n",
      "DDoS      DDoS UDP            Attack    362070\n",
      "          DDoS ICMP           Attack    349699\n",
      "          DDoS TCP            Attack    182598\n",
      "          DDoS SYN            Attack    172397\n",
      "DoS       DoS UDP             Attack    137553\n",
      "          DoS SYN             Attack     98595\n",
      "          DoS ICMP            Attack     98432\n",
      "          DoS TCP             Attack     82096\n",
      "MQTT      DDoS Connect Flood  Attack     41916\n",
      "BENIGN    BENIGN              Benign     37607\n",
      "RECON     Port Scan           Attack     22622\n",
      "MQTT      DoS Publish Flood   Attack      8505\n",
      "          DDoS Publish Flood  Attack      8416\n",
      "RECON     OS Scan             Attack      3834\n",
      "MQTT      DoS Connect Flood   Attack      3131\n",
      "          Malformed Data      Attack      1747\n",
      "SPOOFING  SPOOFING            Attack      1744\n",
      "RECON     Recon VulScan       Attack      1034\n",
      "          Ping Sweep          Attack       186\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def load_dataset_from_structure(root_path):\n",
    "    data = []\n",
    "    for file in root_path.glob('*/*/*.csv'):\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "\n",
    "            category = file.parents[1].name  # DDoS, DoS, etc.\n",
    "            attack = file.parent.name        # DDoS ICMP, DoS TCP, etc.\n",
    "            label_class = 'Benign' if category.upper() == 'BENIGN' else 'Attack'\n",
    "\n",
    "            df['category'] = category\n",
    "            df['attack'] = attack\n",
    "            df['class'] = label_class\n",
    "\n",
    "            data.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to read file {file}: {e}\")\n",
    "    return pd.concat(data, ignore_index=True)\n",
    "\n",
    "# Load train\n",
    "train_root = Path('../../../Data/CICIoMT2024/train')\n",
    "train_df = load_dataset_from_structure(train_root)\n",
    "\n",
    "# Load test\n",
    "test_root = Path('../../../Data/CICIoMT2024/test')\n",
    "test_df = load_dataset_from_structure(test_root)\n",
    "\n",
    "# Cek ringkasan\n",
    "print(\"Train set:\", train_df.shape)\n",
    "print(train_df[['category', 'attack', 'class']].value_counts())\n",
    "print(\"\\nTest set:\", test_df.shape)\n",
    "print(test_df[['category', 'attack', 'class']].value_counts())\n",
    "\n",
    "# Gabungkan train dan test menjadi satu DataFrame\n",
    "df = pd.concat([train_df, test_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e046d679-506f-4573-82ab-5a29833e0596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:216: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Epoch 1/15\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 2.6897\n",
      "Epoch 2/15\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4531\n",
      "Epoch 3/15\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4116\n",
      "Epoch 4/15\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2259\n",
      "Epoch 5/15\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1633\n",
      "Epoch 6/15\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1555\n",
      "Epoch 7/15\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1086\n",
      "Epoch 8/15\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0708\n",
      "Epoch 9/15\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0688\n",
      "Epoch 10/15\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0605\n",
      "Epoch 11/15\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0575\n",
      "Epoch 12/15\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0796\n",
      "Epoch 13/15\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0541\n",
      "Epoch 14/15\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0576\n",
      "Epoch 15/15\n",
      "\u001b[1m278/278\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0521\n",
      "\u001b[1m556/556\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 727us/step\n",
      "\u001b[1m7199/7199\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 657us/step\n",
      "✅ Triplet embedding training selesai.\n"
     ]
    }
   ],
   "source": [
    "# Contrastive Learning + Zero-Day Detection for IoMT IDS (Triplet Loss for Better Embedding)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Lambda\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# === 1. Triplet Loss Function ===\n",
    "def triplet_loss(margin=1.0):\n",
    "    def loss(y_true, y_pred):\n",
    "        anchor, positive, negative = y_pred[:, :32], y_pred[:, 32:64], y_pred[:, 64:]\n",
    "        pos_dist = K.sum(K.square(anchor - positive), axis=1)\n",
    "        neg_dist = K.sum(K.square(anchor - negative), axis=1)\n",
    "        return K.mean(K.maximum(pos_dist - neg_dist + margin, 0.0))\n",
    "    return loss\n",
    "\n",
    "# === 2. Load Data & Sampling Triplets ===\n",
    "if 'class' not in df.columns or 'attack' not in df.columns:\n",
    "    raise ValueError(\"Dataset harus memiliki kolom 'class' dan 'attack'\")\n",
    "\n",
    "df['attack'] = df['attack'].str.upper().str.strip()\n",
    "df['class'] = df['class'].str.upper().str.strip()\n",
    "\n",
    "benign_df = df[df['class'] == 'BENIGN'].copy()\n",
    "spoof_df = df[df['attack'] == 'SPOOFING'].copy()\n",
    "attack_df = df[(df['class'] == 'ATTACK') & (df['attack'] != 'SPOOFING')].copy()\n",
    "\n",
    "triplet_size = min(len(benign_df), len(spoof_df), len(attack_df))\n",
    "triplet_benign = benign_df.sample(n=triplet_size, random_state=42)\n",
    "triplet_attack = attack_df.sample(n=triplet_size, random_state=42)\n",
    "triplet_spoof = spoof_df.sample(n=triplet_size, random_state=42)\n",
    "\n",
    "# === 3. Preprocessing ===\n",
    "feature_cols = df.select_dtypes(include='number').columns\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df[feature_cols])\n",
    "\n",
    "anchor = scaler.transform(triplet_benign[feature_cols])\n",
    "positive = scaler.transform(triplet_attack[feature_cols])\n",
    "negative = scaler.transform(triplet_spoof[feature_cols])\n",
    "\n",
    "X_triplet = np.concatenate([anchor, positive, negative], axis=1)\n",
    "y_dummy = np.zeros((X_triplet.shape[0],))\n",
    "\n",
    "# === 4. Build Triplet Network ===\n",
    "def build_base_network(input_shape):\n",
    "    inp = Input(shape=(input_shape,))\n",
    "    x = Dense(128, activation='relu')(inp)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(32, activation='linear')(x)\n",
    "    return Model(inp, x)\n",
    "\n",
    "input_shape = anchor.shape[1]\n",
    "base_network = build_base_network(input_shape)\n",
    "\n",
    "anchor_input = Input(shape=(input_shape,), name='anchor_input')\n",
    "positive_input = Input(shape=(input_shape,), name='positive_input')\n",
    "negative_input = Input(shape=(input_shape,), name='negative_input')\n",
    "\n",
    "encoded_anchor = base_network(anchor_input)\n",
    "encoded_positive = base_network(positive_input)\n",
    "encoded_negative = base_network(negative_input)\n",
    "\n",
    "merged_output = Lambda(lambda x: K.concatenate(x, axis=1))([encoded_anchor, encoded_positive, encoded_negative])\n",
    "triplet_model = Model(inputs=[anchor_input, positive_input, negative_input], outputs=merged_output)\n",
    "\n",
    "triplet_model.compile(loss=triplet_loss(margin=1.0), optimizer=Adam(0.001))\n",
    "triplet_model.fit(\n",
    "    [anchor, positive, negative], y_dummy,\n",
    "    batch_size=64,\n",
    "    epochs=15\n",
    ")\n",
    "\n",
    "# === 5. Ambil Embedding & Simpan Model Encoder ===\n",
    "encoder = base_network\n",
    "embedding_spoof = encoder.predict(scaler.transform(spoof_df[feature_cols]))\n",
    "embedding_benign = encoder.predict(scaler.transform(benign_df[feature_cols]))\n",
    "\n",
    "# === 6. Evaluasi Visual & Logika Deteksi (dapat dilanjutkan) ===\n",
    "print(\"✅ Triplet embedding training selesai.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5163b5-b617-4143-90e3-dd658a833f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Lambda\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from scipy.spatial import distance\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "# === 6. Anomaly Detection: Isolation Forest, SVM, Mahalanobis ===\n",
    "iso = IsolationForest(contamination=0.05, random_state=42)\n",
    "iso.fit(embedding_benign)\n",
    "iso_pred = [1 if p == -1 else 0 for p in np.concatenate([iso.predict(embedding_benign), iso.predict(embedding_spoof)])]\n",
    "\n",
    "svm = OneClassSVM(kernel='rbf', gamma='auto')\n",
    "svm.fit(embedding_benign)\n",
    "svm_pred = [1 if p == -1 else 0 for p in np.concatenate([svm.predict(embedding_benign), svm.predict(embedding_spoof)])]\n",
    "\n",
    "mean_vec = np.mean(embedding_benign, axis=0)\n",
    "cov_inv = np.linalg.pinv(np.cov(embedding_benign, rowvar=False))\n",
    "d_mahal_benign = [distance.mahalanobis(x, mean_vec, cov_inv) for x in embedding_benign]\n",
    "d_mahal_spoof = [distance.mahalanobis(x, mean_vec, cov_inv) for x in embedding_spoof]\n",
    "thresh = np.percentile(d_mahal_benign, 95)\n",
    "mahal_pred = [1 if d > thresh else 0 for d in d_mahal_benign + d_mahal_spoof]\n",
    "\n",
    "ensemble_pred = [1 if (s == 1 and m == 1) else 0 for s, m in zip(svm_pred, mahal_pred)]\n",
    "y_true = [0] * len(embedding_benign) + [1] * len(embedding_spoof)\n",
    "\n",
    "print(\"\\n[Ensemble Detection Evaluation]\")\n",
    "print(classification_report(y_true, ensemble_pred, target_names=['Benign', 'Spoofing']))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_true, ensemble_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d70041-f2bb-435f-aace-6dfc651ce54b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
