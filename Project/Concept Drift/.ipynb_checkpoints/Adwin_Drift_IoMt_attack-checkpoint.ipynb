{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7149741b-8178-4023-8bca-18452523be02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full ADWIN-based Concept Drift Detection and Evaluation on IoMT IDS Dataset (Enhanced Visualization)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from river import compose, preprocessing, forest, metrics, drift\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2954e493-407e-4c0b-bf1a-6ab61a9c1184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset CICIoMT2024\n",
    "def load_dataset_from_structure(root_path):\n",
    "    data = []\n",
    "    for file in root_path.glob('*/*/*.csv'):\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            category = file.parents[1].name\n",
    "            attack = file.parent.name\n",
    "            label_class = 'Benign' if category.upper() == 'BENIGN' else 'Attack'\n",
    "            df['category'] = category\n",
    "            df['attack'] = attack\n",
    "            df['class'] = label_class\n",
    "            data.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to read file {file}: {e}\")\n",
    "    return pd.concat(data, ignore_index=True)\n",
    "\n",
    "train_root = Path('../../../../Data/CICIoMT2024/train')\n",
    "test_root = Path('../../../../Data/CICIoMT2024/test')\n",
    "df_train = load_dataset_from_structure(train_root)\n",
    "df_test = load_dataset_from_structure(test_root)\n",
    "df = pd.concat([df_train, df_test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcc87bc-7645-40e4-8009-906b3ac95f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode label (attack)\n",
    "le = LabelEncoder()\n",
    "df['attack_encoded'] = le.fit_transform(df['attack'])\n",
    "\n",
    "# Feature processing\n",
    "X = df.drop(['class', 'category', 'attack', 'attack_encoded'], axis=1, errors='ignore').select_dtypes(include=[np.number])\n",
    "y = df['attack_encoded']\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_dicts = pd.DataFrame(X_scaled, columns=X.columns).to_dict(orient='records')\n",
    "y_values = y.tolist()\n",
    "dataset = list(zip(X_dicts, y_values))\n",
    "\n",
    "# Define model\n",
    "model = compose.Pipeline(\n",
    "    ('scaler', preprocessing.StandardScaler()),\n",
    "    ('classifier', forest.ARFClassifier(\n",
    "        seed=42,\n",
    "        n_models=20,\n",
    "        max_features=0.5,\n",
    "        grace_period=30,\n",
    "        leaf_prediction='nb'\n",
    "    ))\n",
    ")\n",
    "\n",
    "# Metrics\n",
    "metric_accuracy = metrics.Accuracy()\n",
    "metric_precision = metrics.WeightedPrecision()\n",
    "metric_recall = metrics.WeightedRecall()\n",
    "metric_f1 = metrics.WeightedF1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3327bf0f-a6ba-456f-bc04-79953c01ec3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drift detector\n",
    "drift_detector = drift.ADWIN()\n",
    "\n",
    "# Logging\n",
    "drift_events = []\n",
    "metric_logs = {'accuracy': [], 'precision': [], 'recall': [], 'f1_score': []}\n",
    "processing_times, drift_time_per_instance, metrics_time_per_instance = [], [], []\n",
    "total_time, drift_time_total, metrics_time_total = 0, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfa0cb2-9087-4a8d-95a6-1a8bd124150a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming loop\n",
    "start_time = time.time()\n",
    "for i, (x, y_true) in enumerate(dataset):\n",
    "    t_start = time.time()\n",
    "    y_pred = model.predict_one(x)\n",
    "    model.learn_one(x, y_true)\n",
    "\n",
    "    t_metrics = time.time()\n",
    "    metric_accuracy.update(y_true, y_pred)\n",
    "    metric_precision.update(y_true, y_pred)\n",
    "    metric_recall.update(y_true, y_pred)\n",
    "    metric_f1.update(y_true, y_pred)\n",
    "    t_metrics_done = time.time()\n",
    "\n",
    "    metric_logs['accuracy'].append(metric_accuracy.get())\n",
    "    metric_logs['precision'].append(metric_precision.get())\n",
    "    metric_logs['recall'].append(metric_recall.get())\n",
    "    metric_logs['f1_score'].append(metric_f1.get())\n",
    "\n",
    "    drift_detector.update(y_pred == y_true)\n",
    "    if drift_detector.drift_detected:\n",
    "        print(f\"Drift detected at instance {i}\")\n",
    "        drift_events.append(i)\n",
    "\n",
    "    processing_times.append(time.time() - t_start)\n",
    "    metrics_time_per_instance.append(t_metrics_done - t_metrics)\n",
    "    drift_time_per_instance.append(time.time() - t_metrics_done)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nTotal execution time: {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Total drifts detected: {len(drift_events)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eb917b-6679-492d-99cd-50a8556b54e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot metric logs\n",
    "instances = list(range(1, len(metric_logs['accuracy']) + 1))\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(instances, metric_logs['accuracy'], label='Accuracy')\n",
    "plt.plot(instances, metric_logs['precision'], label='Precision', linestyle=':')\n",
    "plt.plot(instances, metric_logs['recall'], label='Recall', linestyle='--')\n",
    "plt.plot(instances, metric_logs['f1_score'], label='F1-Score', linestyle='-.')\n",
    "for i, drift in enumerate(drift_events):\n",
    "    plt.axvline(x=drift, color='blue', linestyle=':', linewidth=1, label='Drift Event' if i == 0 else \"\")\n",
    "plt.xlabel('Instances Processed')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('metrics_drift_adwin.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15ac7dc-3f74-42ca-b054-52a9f748da0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy before and after drift (Figure 16 style)\n",
    "if drift_events:\n",
    "    pre_drift_acc = [metric_logs['accuracy'][i - 1] for i in drift_events if i > 0]\n",
    "    post_drift_acc = [metric_logs['accuracy'][i] for i in drift_events if i < len(metric_logs['accuracy'])]\n",
    "    drift_labels = [f\"Drift {i+1}\" for i in range(len(pre_drift_acc))]\n",
    "\n",
    "    x = range(len(pre_drift_acc))\n",
    "    bar_width = 0.4\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(x, pre_drift_acc, width=bar_width, label='Pre-Drift Accuracy', alpha=0.7)\n",
    "    plt.bar([i + bar_width for i in x], post_drift_acc, width=bar_width, label='Post-Drift Accuracy', alpha=0.7)\n",
    "\n",
    "    for i, (pre, post) in enumerate(zip(pre_drift_acc, post_drift_acc)):\n",
    "        plt.text(i, pre + 0.005, f'{pre:.3f}', ha='center')\n",
    "        plt.text(i + bar_width, post + 0.005, f'{post:.3f}', ha='center')\n",
    "\n",
    "    plt.xticks([i + bar_width / 2 for i in x], drift_labels)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Drift Points')\n",
    "    plt.title('Model Accuracy Before and After Drift')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "    plt.savefig('drift_accuracy_comparison.png', dpi=300)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No drift events detected, skipping pre/post drift comparison.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da87d20-9045-4548-8849-9e7a8e959a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final metrics\n",
    "print(f\"Final Accuracy: {metric_accuracy.get():.4f}\")\n",
    "print(f\"Final Precision: {metric_precision.get():.4f}\")\n",
    "print(f\"Final Recall: {metric_recall.get():.4f}\")\n",
    "print(f\"Final F1-Score: {metric_f1.get():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6ba5c8-09dc-475b-b343-d3933013317e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
